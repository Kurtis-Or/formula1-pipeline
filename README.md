# Azure Databricks & PySpark Data Engineering Project

## ğŸ“Œ Project Overview
This repository showcases my hands-on experience with **Azure Databricks, Azure Data Factory, and PySpark**

The project focuses on building an **end-to-end data pipeline** using **Azure Databricks, Azure Data Factory, PySpark, and Delta Lake**, demonstrating skills in **data ingestion, transformation, and optimization**.

## ğŸ—ï¸ Technologies Used
- **Azure Databricks** - Managed Spark environment
- **Apache Spark (PySpark)** - Distributed data processing
- **Delta Lake** - ACID transactions for data lakes
- **Azure Data Lake Storage (ADLS)** - Cloud-based storage
- **Azure Blob Storage** - Data storage and ingestion
- **Azure DevOps/GitHub** - Version control and CI/CD
- **Python & SQL** - Data transformations

## ğŸ› ï¸ Project Architecture

![f1_architecture](https://github.com/user-attachments/assets/7a1ba4b3-fd8c-481a-8eeb-f5143bb51af3)

## ğŸ“‚ Repository Structure
```
ğŸ“ formula1-pipeline
â”‚â”€â”€ ğŸ“ analysis/           # Short Analysis of data
â”‚â”€â”€ ğŸ“ includes/           # Common functions used and configurations
â”‚â”€â”€ ğŸ“ ingestion/          # All of the notebooks used to ingest data
â”‚â”€â”€ ğŸ“ raw/                # Raw data creation notebook
â”‚â”€â”€ ğŸ“ transformations/    # Notebooks for processing all of the raw data
â”‚â”€â”€ ğŸ“ utils/              # Incremental Load script
â”‚â”€â”€ README.md              # Project overview and setup instructions
â”‚â”€â”€ LICENSE                # License for the repository
```

## ğŸš€ Features Implemented
âœ… Data ingestion from Azure Blob Storage using **PySpark**  
âœ… Data transformation using **Spark SQL & DataFrames**  
âœ… Storing and optimizing data using **Delta Lake**  
âœ… Writing partitioned Parquet files for better performance  
âœ… Handling schema evolution and ACID transactions  
âœ… Performance tuning with **Spark optimizations**  
âœ… **Automated pipeline orchestration with Azure Data Factory (ADF)**  
âœ… **Triggers for scheduled and event-based execution**  

## ğŸ“ˆ Data Pipeline Overview
The project implements a **scalable data pipeline** following these steps:

1. **Data Ingestion**: Raw data is ingested from **Azure Blob Storage** using **Databricks Autoloader** or **Spark APIs**.
2. **Data Cleaning & Transformation**: Using **PySpark DataFrames**, data is transformed, deduplicated, and enriched.
3. **Data Storage & Processing**:
   - Processed data is written to **Delta Lake** in **Azure Data Lake Storage (ADLS)**.
4. **Incremental Processing**: Change Data Capture (CDC) is handled using **Merge into** statements in **Delta Lake**.
5. **Performance Optimization**: Caching, Bucketing, and AQE (Adaptive Query Execution) are implemented.
6. **Pipeline Orchestration & Triggers**:
   - Azure Data Factory **pipelines** automate data movement and transformation.
   - **Triggers** are configured for Tumbling Windows as all of the data is historical and no longer being updated
7. **Data Export**: Data is written to **Parquet files** and exposed to downstream analytics platforms.

This pipeline ensures **data reliability, scalability, automation, and efficiency** using best practices in **Apache Spark, Databricks, and Azure Data Factory**.
